{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\(in)definite.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\adjectives.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\adverbs.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\animals.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\art.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\clothes.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\colors.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\conjug_past.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\conjug_present.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\determiners.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\economy.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\education.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\emotions.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\environment.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\family.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\femalenames.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\food.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\health.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\humanbody.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\idioms.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\imagenet_b_darija.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\imperatives.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\malenames.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\masculine_feminine_plural.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\nouns.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\numbers.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\places.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\plants.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\possessives.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\prepositions.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\professions.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\pronouns.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\proverbs.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\religion.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\sentences.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\sentences2.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\sport.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\technology.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\time.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\utils.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\verb-to-noun.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\verbs.csv\n",
      "C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\weird.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk(r'C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'english':[],\n",
    "       'darija':[]}\n",
    "filename = r\"C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\sentences.csv\"\n",
    "with open(filename) as file:\n",
    "    for line in file:\n",
    "        if '\"' in line:\n",
    "            line1 = line.split('\"')\n",
    "            if len(line1)>=2:\n",
    "                data['english'].append(line1[1])\n",
    "                data['darija'].append(line1[len(line1)-2])\n",
    "        else:\n",
    "            line2 = line.split(',')\n",
    "            if len(line2)>=2:\n",
    "                data['english'].append(line2[0])\n",
    "                data['darija'].append(line2[1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1037\n",
      "1037\n"
     ]
    }
   ],
   "source": [
    "print(len(data['english']))\n",
    "print(len(data['darija']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>darija</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>english</td>\n",
       "      <td>darija</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They're hiding something, I'm sure!</td>\n",
       "      <td>homa mkhbbyin chi haja, ana mti99en!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's obvious they're trying to keep their cool.</td>\n",
       "      <td>bayna homa tay7awlo ib9aw mbrrdin.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the hotels don't seem very comfortable.</td>\n",
       "      <td>loTilat mabaynach fihom mori7in bzzaf.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>he is probably about to be laid off by head of...</td>\n",
       "      <td>ghaliban ghayjrriw 3lih mn lkhdma!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0                                            english   \n",
       "1                They're hiding something, I'm sure!   \n",
       "2    It's obvious they're trying to keep their cool.   \n",
       "3            the hotels don't seem very comfortable.   \n",
       "4  he is probably about to be laid off by head of...   \n",
       "\n",
       "                                   darija  \n",
       "0                                  darija  \n",
       "1    homa mkhbbyin chi haja, ana mti99en!  \n",
       "2      bayna homa tay7awlo ib9aw mbrrdin.  \n",
       "3  loTilat mabaynach fihom mori7in bzzaf.  \n",
       "4      ghaliban ghayjrriw 3lih mn lkhdma!  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1037,), dtype=string, numpy=\n",
       "array([b'darija', b'homa mkhbbyin chi haja, ana mti99en!',\n",
       "       b'bayna homa tay7awlo ib9aw mbrrdin.', ..., b'', b'', b''],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english = df['english']\n",
    "darija = df['darija']\n",
    "tf.convert_to_tensor(english)\n",
    "tf.convert_to_tensor(darija)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bye'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english[654]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lay3awn'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "darija[654]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((english, darija))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "def batch_iterator():\n",
    "    for i in range(0,len(df),batch_size):\n",
    "        yield df[i:i+batch_size][\"darija\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ec1af543ee4dfeac7b2212782b1c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467103a67a3f495fa4a8bcb0bb39ae69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e78a07e118d4ce59f716b03a4fe357d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2f7633b8484917b94ee172d9021981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "darija_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(),vocab_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('darija-tokenizer\\\\tokenizer_config.json',\n",
       " 'darija-tokenizer\\\\special_tokens_map.json',\n",
       " 'darija-tokenizer\\\\vocab.txt',\n",
       " 'darija-tokenizer\\\\added_tokens.json',\n",
       " 'darija-tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "darija_tokenizer.save_pretrained(\"darija-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 64\n",
    "tokens = darija_tokenizer(df['darija'].tolist(), max_length = seq_len,\n",
    "                  truncation = True, padding='max_length',add_special_tokens=True,\n",
    "                  return_tensors='np')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 3746,  653, 4286, 2027, 3704,    9,    3,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>specific1_darija</th>\n",
       "      <th>specific2_darija</th>\n",
       "      <th>general_darija</th>\n",
       "      <th>darija_ar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kit_fox</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ta3leb</td>\n",
       "      <td>t3leb</td>\n",
       "      <td>تعلب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English_setter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kelb</td>\n",
       "      <td>كلب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siberian_husky</td>\n",
       "      <td>husky dial siberia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kelb</td>\n",
       "      <td>كلب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Australian_terrier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kelb</td>\n",
       "      <td>كلب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>English_springer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kelb</td>\n",
       "      <td>كلب</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              english    specific1_darija specific2_darija general_darija  \\\n",
       "0             kit_fox                 NaN           ta3leb          t3leb   \n",
       "1      English_setter                 NaN              NaN           kelb   \n",
       "2      Siberian_husky  husky dial siberia              NaN           kelb   \n",
       "3  Australian_terrier                 NaN              NaN           kelb   \n",
       "4    English_springer                 NaN              NaN           kelb   \n",
       "\n",
       "  darija_ar  \n",
       "0     تعلب   \n",
       "1      كلب   \n",
       "2      كلب   \n",
       "3      كلب   \n",
       "4      كلب   "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\imagenet_b_darija.csv\", delimiter=',', encoding='utf8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def removePunctuation(x):\n",
    "    x = x.lower()\n",
    "    x = re.sub(r'[^\\x00-\\x7f]',r' ',x)\n",
    "    x = x.replace('\\r','')\n",
    "    x = x.replace('\\n','')\n",
    "    x = x.replace('  ','')\n",
    "    x = x.replace('\\'','')\n",
    "    return re.sub(\"[\"+string.punctuation+\"]\", \" \", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'or', 'and', 'haven', 's', 'the', \"you've\", 'very', 'all', 'mightn', 'y', 'have', 'be', 'for', 'been', 'were', 'can', 'some', 'yours', 'we', 'other', 'so', 'these', \"didn't\", 'hers', \"hasn't\", 'yourself', 'you', 'doing', 'below', 'couldn', 'was', \"shouldn't\", 'has', 'against', 'itself', 'if', 'up', \"weren't\", 'with', \"wasn't\", 'whom', 'into', 'more', 'should', 'in', 'how', 'didn', 'd', 'ain', \"she's\", \"mightn't\", 'he', 'few', 'wouldn', 'don', 'before', 'to', 're', 'down', 'their', 'do', 'above', \"shan't\", 'while', 'by', 'where', \"you'll\", 'of', 'again', 'she', 'hasn', \"should've\", 'aren', 'herself', 'wasn', 'that', 'an', 'most', 'at', 'no', 'me', 'is', 'am', 'what', 'off', \"mustn't\", 'out', 'weren', 'they', 'himself', \"wouldn't\", 'ours', 'er', 'through', 'mustn', 'won', \"hadn't\", 'will', 'theirs', 'nor', 'then', 'which', 'being', 'm', 'its', 'now', \"haven't\", \"don't\", 'him', 'needn', 'isn', 'll', 'after', 'this', 'had', 'same', 'on', 'only', 'a', 'as', \"couldn't\", 'just', \"you'd\", 'not', 'yourselves', 'there', 'her', 'over', \"that'll\", 'own', 'but', 'my', 'them', 't', \"you're\", \"needn't\", 'having', \"it's\", 'ourselves', 'until', 'under', 'once', \"isn't\", 'hadn', 'shouldn', 'who', 'than', \"aren't\", 'doesn', 'because', 'are', 'does', 'any', 'our', 'your', 'it', 'when', 'too', 'o', 'did', 'further', 'those', 'both', 'such', 'themselves', \"won't\", 'about', 'myself', 'each', 'why', 'during', 'ma', 'between', 've', 'from', 'i', 'shan', 'here', 'his', \"doesn't\"}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#add words that aren't in the NLTK stopwords list\n",
    "new_stopwords = ['and', 'at', 'so', 'er']\n",
    "new_stopwords_list = stop_words.union(new_stopwords)\n",
    "\n",
    "#remove words that are in NLTK stopwords list\n",
    "not_stopwords = {'snake', 'lion', 'pigs'} \n",
    "final_stop_words = set([word for word in new_stopwords_list if word not in not_stopwords])\n",
    "\n",
    "print(final_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(x):\n",
    "    x= removePunctuation(x)\n",
    "    #x= removeStopwords(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               [kit, fox]\n",
       "1        [english, setter]\n",
       "2        [siberian, husky]\n",
       "3    [australian, terrier]\n",
       "4      [english, springer]\n",
       "5            [grey, whale]\n",
       "6          [lesser, panda]\n",
       "7          [egyptian, cat]\n",
       "8                   [ibex]\n",
       "9           [persian, cat]\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "tin = pd.Series([word_tokenize(processText(x)) for x in df['english']])\n",
    "tin.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [ \"hurry and you will be late\",\"a man bitten by a snake is afraid of rope\", \"he knocks and says who is there\", \"a friend in need is a friend indeed\", \"better luck next time\"]\n",
    "\n",
    "vocab = [s.encode('utf-8').split() for s in sentences]\n",
    "\n",
    "voc_vec = word2vec.Word2Vec(vocab, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 4   # Minimum word count #Changed 40 to 4 only avoid build a vocabulary 1st                       \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "model = word2vec.Word2Vec(tin, workers=num_workers, \n",
    "                          min_count = min_word_count,\n",
    "                          window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "import logging\n",
    "from timeit import default_timer\n",
    "import threading\n",
    "from six.moves import range\n",
    "from six import itervalues, string_types\n",
    "from gensim import matutils\n",
    "from numpy import float32 as REAL, ones, random, dtype\n",
    "from types import GeneratorType\n",
    "from gensim.utils import deprecated\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None):\n",
    "        \"\"\"Deprecated, use self.wv.most_similar() instead.\n",
    "        Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
    "        \"\"\"\n",
    "        return self.wv.most_similar(positive, negative, topn, restrict_vocab, indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\adnane\\Desktop\\ORANGECHAT2\\darija-data\\sentences2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>darija</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is not a good start to the day</td>\n",
       "      <td>Hadi machi bedia zina l nhar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As you say.</td>\n",
       "      <td>klamk howa lkbir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I ve got so much to do today, you know there a...</td>\n",
       "      <td>3ndi bezzaf maydar lyouma, rak 3arf lw9t mziye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let's see if anything happens quickly.</td>\n",
       "      <td>nchoufo ila trat chi 7aja b zerba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I wish I had climbed the stairs</td>\n",
       "      <td>kantmenna koun tl3t f droj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0                This is not a good start to the day   \n",
       "1                                        As you say.   \n",
       "2  I ve got so much to do today, you know there a...   \n",
       "3             Let's see if anything happens quickly.   \n",
       "4                    I wish I had climbed the stairs   \n",
       "\n",
       "                                              darija  \n",
       "0                       Hadi machi bedia zina l nhar  \n",
       "1                                   klamk howa lkbir  \n",
       "2  3ndi bezzaf maydar lyouma, rak 3arf lw9t mziye...  \n",
       "3                  nchoufo ila trat chi 7aja b zerba  \n",
       "4                         kantmenna koun tl3t f droj  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming the CSV has two columns: 'English' and 'Darija'\n",
    "english_sentences = data['english'].tolist()\n",
    "darija_sentences = data['darija'].tolist()\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_eng, temp_eng, train_dar, temp_dar = train_test_split(english_sentences, darija_sentences, test_size=0.2)\n",
    "val_eng, test_eng, val_dar, test_dar = train_test_split(temp_eng, temp_dar, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer_eng = Tokenizer()\n",
    "tokenizer_eng.fit_on_texts(train_eng)\n",
    "train_eng_seq = tokenizer_eng.texts_to_sequences(train_eng)\n",
    "val_eng_seq = tokenizer_eng.texts_to_sequences(val_eng)\n",
    "\n",
    "tokenizer_dar = Tokenizer()\n",
    "tokenizer_dar.fit_on_texts(train_dar)\n",
    "train_dar_seq = tokenizer_dar.texts_to_sequences(train_dar)\n",
    "val_dar_seq = tokenizer_dar.texts_to_sequences(val_dar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# Convert the data to lowercase and remove any special characters\n",
    "data['english'] = data['english'].str.lower().str.replace('[^\\w\\s]', '', regex=True)\n",
    "data['darija'] = data['darija'].str.lower().str.replace('[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# Add special tokens to the sentences\n",
    "data['english'] = data['english'].apply(lambda x: 'starttoken ' + x + ' endtoken')\n",
    "data['darija'] = data['darija'].apply(lambda x: 'starttoken ' + x + ' endtoken')\n",
    "\n",
    "# Initialize tokenizers and add special tokens explicitly\n",
    "english_tokenizer = Tokenizer(filters='')\n",
    "darija_tokenizer = Tokenizer(filters='')\n",
    "\n",
    "# Manually add the special tokens\n",
    "english_tokenizer.fit_on_texts(['starttoken', 'endtoken'])\n",
    "darija_tokenizer.fit_on_texts(['starttoken', 'endtoken'])\n",
    "\n",
    "# Fit the tokenizers on the actual text data\n",
    "english_tokenizer.fit_on_texts(data['english'])\n",
    "darija_tokenizer.fit_on_texts(data['darija'])\n",
    "\n",
    "# Convert text to sequences\n",
    "english_sequences = english_tokenizer.texts_to_sequences(data['english'])\n",
    "darija_sequences = darija_tokenizer.texts_to_sequences(data['darija'])\n",
    "\n",
    "\n",
    "\n",
    "# Pad the sequences\n",
    "max_eng = [len(seq) for seq in english_sequences]\n",
    "max_english_length = 0\n",
    "for i in max_eng:\n",
    "    if i>max_english_length:\n",
    "        max_english_length=i\n",
    "\n",
    "max_dar = [len(seq) for seq in darija_sequences]\n",
    "max_darija_length = 0\n",
    "for i in max_dar:\n",
    "    if i>max_darija_length:\n",
    "        max_darija_length=i\n",
    "        \n",
    "english_sequences = pad_sequences(english_sequences, maxlen=max_english_length, padding='post')\n",
    "darija_sequences = pad_sequences(darija_sequences, maxlen=max_darija_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(english_sequences, darija_sequences, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\adnane\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\adnane\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 40)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 31)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 40, 256)              305152    ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 31, 256)              443904    ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 256),                525312    ['embedding[0][0]']           \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, 31, 256),            525312    ['embedding_1[0][0]',         \n",
      "                              (None, 256),                           'lstm[0][1]',                \n",
      "                              (None, 256)]                           'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 31, 1734)             445638    ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2245318 (8.57 MB)\n",
      "Trainable params: 2245318 (8.57 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "\n",
    "# Define the encoder\n",
    "encoder_inputs = Input(shape=(max_english_length,))\n",
    "encoder_embedding = Embedding(len(english_tokenizer.word_index) + 1, 256, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm, state_h, state_c = LSTM(256, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the decoder\n",
    "decoder_inputs = Input(shape=(max_darija_length,))\n",
    "decoder_embedding = Embedding(len(darija_tokenizer.word_index) + 1, 256, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(darija_tokenizer.word_index) + 1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.0129 - val_loss: 8.1453\n",
      "Epoch 2/50\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.0112 - val_loss: 8.1810\n",
      "Epoch 3/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0095 - val_loss: 8.2122\n",
      "Epoch 4/50\n",
      "146/146 [==============================] - 8s 51ms/step - loss: 0.0093 - val_loss: 8.2367\n",
      "Epoch 5/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0085 - val_loss: 8.2624\n",
      "Epoch 6/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0076 - val_loss: 8.2830\n",
      "Epoch 7/50\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 0.0072 - val_loss: 8.3069\n",
      "Epoch 8/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0071 - val_loss: 8.3276\n",
      "Epoch 9/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0065 - val_loss: 8.3525\n",
      "Epoch 10/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0066 - val_loss: 8.3657\n",
      "Epoch 11/50\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.0054 - val_loss: 8.3923\n",
      "Epoch 12/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0049 - val_loss: 8.4137\n",
      "Epoch 13/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0057 - val_loss: 8.4326\n",
      "Epoch 14/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0046 - val_loss: 8.4508\n",
      "Epoch 15/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0049 - val_loss: 8.4729\n",
      "Epoch 16/50\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.0043 - val_loss: 8.4931\n",
      "Epoch 17/50\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 0.0039 - val_loss: 8.5108\n",
      "Epoch 18/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0050 - val_loss: 8.5274\n",
      "Epoch 19/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0035 - val_loss: 8.5575\n",
      "Epoch 20/50\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.0038 - val_loss: 8.5725\n",
      "Epoch 21/50\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 0.0038 - val_loss: 8.5924\n",
      "Epoch 22/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0037 - val_loss: 8.6123\n",
      "Epoch 23/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0045 - val_loss: 8.6233\n",
      "Epoch 24/50\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.0026 - val_loss: 8.6569\n",
      "Epoch 25/50\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.0028 - val_loss: 8.6755\n",
      "Epoch 26/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0030 - val_loss: 8.6934\n",
      "Epoch 27/50\n",
      "146/146 [==============================] - 8s 51ms/step - loss: 0.0031 - val_loss: 8.7219\n",
      "Epoch 28/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0030 - val_loss: 8.7283\n",
      "Epoch 29/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0031 - val_loss: 8.7737\n",
      "Epoch 30/50\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 0.0079 - val_loss: 8.7885\n",
      "Epoch 31/50\n",
      "146/146 [==============================] - 7s 51ms/step - loss: 0.1396 - val_loss: 8.5139\n",
      "Epoch 32/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0994 - val_loss: 8.5452\n",
      "Epoch 33/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0287 - val_loss: 8.6254\n",
      "Epoch 34/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0102 - val_loss: 8.6959\n",
      "Epoch 35/50\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 0.0050 - val_loss: 8.7366\n",
      "Epoch 36/50\n",
      "146/146 [==============================] - 7s 51ms/step - loss: 0.0036 - val_loss: 8.7613\n",
      "Epoch 37/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0035 - val_loss: 8.7868\n",
      "Epoch 38/50\n",
      "146/146 [==============================] - 7s 51ms/step - loss: 0.0031 - val_loss: 8.8114\n",
      "Epoch 39/50\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 0.0035 - val_loss: 8.8399\n",
      "Epoch 40/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0039 - val_loss: 8.8516\n",
      "Epoch 41/50\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 0.0034 - val_loss: 8.8749\n",
      "Epoch 42/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0033 - val_loss: 8.8929\n",
      "Epoch 43/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0021 - val_loss: 8.9144\n",
      "Epoch 44/50\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.0034 - val_loss: 8.9310\n",
      "Epoch 45/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0022 - val_loss: 8.9492\n",
      "Epoch 46/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0022 - val_loss: 8.9666\n",
      "Epoch 47/50\n",
      "146/146 [==============================] - 7s 51ms/step - loss: 0.0022 - val_loss: 8.9826\n",
      "Epoch 48/50\n",
      "146/146 [==============================] - 8s 52ms/step - loss: 0.0028 - val_loss: 9.0039\n",
      "Epoch 49/50\n",
      "146/146 [==============================] - 9s 63ms/step - loss: 0.0025 - val_loss: 9.0195\n",
      "Epoch 50/50\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.0021 - val_loss: 9.0444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a147275b50>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare target data by shifting the sequences\n",
    "y_train_shifted = np.zeros_like(y_train)\n",
    "y_train_shifted[:, :-1] = y_train[:, 1:]\n",
    "\n",
    "y_val_shifted = np.zeros_like(y_val)\n",
    "y_val_shifted[:, :-1] = y_val[:, 1:]\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train, y_train], y_train_shifted, validation_data=([X_val, y_val], y_val_shifted), epochs=50, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 36ms/step - loss: 9.2050\n",
      "Validation Loss: 9.20496654510498\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "loss = model.evaluate([X_val, y_val], y_val_shifted)\n",
    "print(f'Validation Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "katban lia\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence):\n",
    "    # Preprocess the sentence\n",
    "    sentence = 'starttoken ' + sentence.lower().replace('[^\\w\\s]', '') + ' endtoken'\n",
    "    sequence = english_tokenizer.texts_to_sequences([sentence])\n",
    "    sequence = pad_sequences(sequence, maxlen=max_english_length, padding='post')\n",
    "\n",
    "    # Encode the sentence\n",
    "    states = encoder_model.predict(sequence)\n",
    "    \n",
    "    # Initialize the target sequence with the start token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = darija_tokenizer.word_index['starttoken']\n",
    "    \n",
    "    translated_sentence = []\n",
    "    \n",
    "    for _ in range(max_darija_length):\n",
    "        # Predict the next word\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states)\n",
    "        \n",
    "        # Get the highest probability word index\n",
    "        word_id = np.argmax(output_tokens[0, -1, :])\n",
    "        word = darija_tokenizer.index_word.get(word_id, '')\n",
    "        \n",
    "        if word == 'endtoken':\n",
    "            break\n",
    "        \n",
    "        translated_sentence.append(word)\n",
    "        \n",
    "        # Update the target sequence and states\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = word_id\n",
    "        states = [h, c]\n",
    "\n",
    "    return ' '.join(translated_sentence)\n",
    "\n",
    "# Define the encoder model for prediction\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define the decoder model for prediction\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Translate a sample sentence\n",
    "print(translate('how i can help you?').replace('starttoken ',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "model.save('translation_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save English tokenizer\n",
    "with open('english_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(english_tokenizer, f)\n",
    "\n",
    "# Save Darija tokenizer\n",
    "with open('darija_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(darija_tokenizer, f)\n",
    "\n",
    "# Save maximum sequence lengths\n",
    "with open('max_seq_lengths.pkl', 'wb') as f:\n",
    "    pickle.dump({'max_english_length': max_english_length, 'max_darija_length': max_darija_length}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
